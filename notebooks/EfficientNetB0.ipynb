{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mount the Folder from Google Colab\n",
        "\n",
        "This section mounts your Google Drive folder to access data and resources directly from Colab.  \n",
        "You may not need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dn2fLRUPKeW",
        "outputId": "3db12772-c405-4741-a902-db63f3e27532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Required Libraries\n",
        "\n",
        "This section imports all necessary libraries and modules for data loading, preprocessing, model building, training, and evaluation. Ensure all dependencies are installed before running the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GwdhwCGFO6Vs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision.ops import SqueezeExcitation\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95evhpQQPsWw"
      },
      "outputs": [],
      "source": [
        "# archive_path = '[it was google drive path]'\n",
        "\n",
        "train_length, valid_length = 500, 72\n",
        "\n",
        "# train_labels = json.loads(open(f'{archive_path}/data/train/annotations').read())\n",
        "# valid_labels = json.loads(open(f'{archive_path}/data/valid/annotations').read())\n",
        "\n",
        "train_labels = json.loads(open('../data/train/annotations').read())\n",
        "valid_labels = json.loads(open('../data/valid/annotations').read())\n",
        "\n",
        "# 0: turtle and 1: penguin\n",
        "train_labels = [0 if item[\"category_id\"] == 2 else 1 for item in train_labels]\n",
        "valid_labels = [0 if item[\"category_id\"] == 2 else 1 for item in valid_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Image normalization parameters\n",
        "Check 'data_preprocessing_and_analysis.ipynb' for details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4i2D3hQVPxyf"
      },
      "outputs": [],
      "source": [
        "mean = [0.6012, 0.6324, 0.6453]\n",
        "std = [0.2472, 0.2300, 0.2304]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Datasets and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0kzDi5qRYf_g"
      },
      "outputs": [],
      "source": [
        "class PenguinVSTurtleDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for the Penguin vs Turtle classification task.\n",
        "\n",
        "    Args:\n",
        "        length (int): Length of the dataset.\n",
        "        transform (callable): Transform to be applied on the images.\n",
        "        annotations (list): List of image annotations.\n",
        "        train (bool): Flag indicating if the dataset is for training or validation.\n",
        "    \"\"\"\n",
        "    def __init__(self, length, annotations, train=True):\n",
        "        super().__init__()\n",
        "        self.len = length\n",
        "        self.annotations = annotations\n",
        "        self.train = train\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)), # Image resizing: 224 x 224 x 3\n",
        "            transforms.RandomHorizontalFlip(), # Data augmentation: Random horizontal flip\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std) # Normalization\n",
        "        ]) if train else transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        path = f'{archive_path}/data/train/images/{id}.jpg' if self.train else f'{archive_path}/data/valid/images/{id}.jpg'\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        label = train_labels[id] if self.train else valid_labels[id]\n",
        "\n",
        "        return (image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eeQMhenOYmqK"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_set = PenguinVSTurtleDataset(train_length, train_labels, train=True)\n",
        "valid_set = PenguinVSTurtleDataset(valid_length, valid_labels, train=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Architecutre: EfficientNetB0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MBConv Blocks\n",
        "[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/pdf/1801.04381)  \n",
        "[Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7GIp4tSPYoEZ"
      },
      "outputs": [],
      "source": [
        "class MBConv(nn.Module):\n",
        "    \"\"\"\n",
        "    MBConv block for EfficientNet-B0\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "        kernel_size (int): Size of the convolutional kernel.\n",
        "        stride (int): Stride for the convolutional layers.\n",
        "        expand_ratio (int): Expansion ratio for the bottleneck.\n",
        "        drop_out (float): Dropout rate.\n",
        "        survival_prob (float): Survival probability for stochastic depth.\n",
        "        se_ratio (float): Squeeze-and-Excitation ratio.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, expand_ratio: int, drop_out: float = 0.2, survival_prob: float = 0.8, se_ratio: float = 0.25):\n",
        "        super(MBConv, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.se_ratio = se_ratio\n",
        "        # Survival probability for stochastic depth.\n",
        "        self.survival_prob = survival_prob\n",
        "\n",
        "        # Residual Connection Possibility\n",
        "        self.residual = self.stride == 1 and in_channels == out_channels\n",
        "\n",
        "        expanded_channels = in_channels * expand_ratio\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # 1. Expansion Layer (1x1 Conv)\n",
        "        if expand_ratio > 1:\n",
        "            layers.extend([\n",
        "                nn.Conv2d(in_channels, expanded_channels, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(expanded_channels),\n",
        "                nn.SiLU(inplace=True)\n",
        "            ])\n",
        "\n",
        "        # 2. Depthwise Convolution (3x3 Conv)\n",
        "        layers.extend([\n",
        "            nn.Conv2d(expanded_channels, expanded_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expanded_channels, bias=False),\n",
        "            nn.BatchNorm2d(expanded_channels),\n",
        "            nn.SiLU(inplace=True)\n",
        "        ])\n",
        "\n",
        "        # 3. Squeeze-and-Excitation Layer\n",
        "        if self.se_ratio is not None:\n",
        "            layers.append(SqueezeExcitation(input_channels=expanded_channels, squeeze_channels=int(expanded_channels * se_ratio)))\n",
        "\n",
        "        # 4. Projection Layer (1x1 Conv)\n",
        "        layers.extend([\n",
        "            nn.Conv2d(expanded_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Dropout(drop_out)  # Dropout layer with a rate of 0.2\n",
        "        ])\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "        # self.stochastic_depth = StochasticDepth(p=self.survival_prob, mode=\"row\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, out_channels, height, width)\n",
        "        \"\"\"\n",
        "        if self.residual:\n",
        "            p = self.survival_prob\n",
        "            if np.random.choice([0, 1], p=[1-p, p]) == 1:\n",
        "                out = self.block(x) + x\n",
        "            else:\n",
        "                out = x\n",
        "        else:\n",
        "            out = self.block(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Main Architecture\n",
        "[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "izrX7oMwYpbI"
      },
      "outputs": [],
      "source": [
        "class EfficientNetB0(nn.Module):\n",
        "    \"\"\"\n",
        "    EfficientNet-B0 model\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of output classes.\n",
        "        stochastic_depth_prob (float): Stochastic depth probability.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int, stochastic_depth_prob: float=0.8):\n",
        "        super(EfficientNetB0, self).__init__()\n",
        "\n",
        "        # (out_channels, expansion_factor, kernel_size, stride, repeats)\n",
        "        config: list[tuple[int, int, int, int]] = [\n",
        "            (16, 1, 3, 1, 1),\n",
        "            (24, 6, 3, 2, 2),\n",
        "            (40, 6, 5, 2, 2),\n",
        "            (80, 6, 3, 2, 3),\n",
        "            (112, 6, 5, 1, 3),\n",
        "            (192, 6, 5, 2, 4),\n",
        "            (320, 6, 3, 1, 1),\n",
        "        ]\n",
        "\n",
        "        # 1. Initial Convolution Layer (Stem)\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.SiLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 2. MBConv Blocks\n",
        "        blocks = []\n",
        "        in_channels = 32\n",
        "        block_idx = 0\n",
        "        for out_channels, expand_ratio, kernel_size, stride, repeats in config:\n",
        "            for i in range(repeats):\n",
        "                current_stride = stride if i == 0 else 1\n",
        "                blocks.append(MBConv(in_channels, out_channels, kernel_size, current_stride, expand_ratio, survival_prob=stochastic_depth_prob))\n",
        "                in_channels = out_channels\n",
        "                block_idx += 1\n",
        "\n",
        "        self.main_blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        # 3. Head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(320, 1280, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(1280),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),  # Global Average Pooling\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1280, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, 3, height, width)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        x = self.stem(x)\n",
        "        x = self.main_blocks(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training Setup and the Architecture Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsm5CN48YsHs",
        "outputId": "843a49d1-081d-46ef-af6c-23f5afa9a544"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EfficientNetB0(\n",
              "  (stem): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): SiLU(inplace=True)\n",
              "  )\n",
              "  (main_blocks): Sequential(\n",
              "    (0): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (4): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (6): SiLU(inplace=True)\n",
              "        (7): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(36, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
              "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(36, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "        (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(60, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "        (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(60, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (6): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "        (4): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (7): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "        (4): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (8): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "        (4): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (9): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "        (4): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (10): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "        (4): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (11): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "        (4): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (12): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (13): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (14): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (15): MBConv(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU(inplace=True)\n",
              "        (3): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
              "        (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU(inplace=True)\n",
              "        (6): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Sigmoid()\n",
              "        )\n",
              "        (7): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (8): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (9): SiLU(inplace=True)\n",
              "        (10): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): SiLU(inplace=True)\n",
              "    (3): AdaptiveAvgPool2d(output_size=1)\n",
              "    (4): Flatten(start_dim=1, end_dim=-1)\n",
              "    (5): Linear(in_features=1280, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EfficientNetB0(num_classes=2, stochastic_depth_prob=0.2)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions to train & evaluate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l4jo3TAHfckS"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, valid_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    average_loss = running_loss / len(valid_loader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    print(f\"Validation Loss: {average_loss:.4f} - Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    true_positives = cm[1, 1]\n",
        "    false_positives = cm[0, 1]\n",
        "    false_negatives = cm[1, 0]\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "\n",
        "\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    # Calculate F1-score, handling the case where precision + recall is zero\n",
        "    if (precision + recall) == 0:\n",
        "        f1_score = 0\n",
        "    else:\n",
        "        f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "    print(f'F1-score: {f1_score}')\n",
        "\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, info):\n",
        "    best_accuracy = 0\n",
        "    best_accuracy_valid = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "            print('.', end='', flush=True)\n",
        "\n",
        "        average_loss = running_loss / len(train_loader)\n",
        "        accuracy = correct_predictions / total_samples\n",
        "\n",
        "        print(f\"\\n --- Epoch {epoch + 1}/{num_epochs} - Train Loss: {average_loss:.4f} - Train Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), f\"{info}_train.pth\")\n",
        "\n",
        "        valid_accuracy = evaluate(model, valid_loader, criterion)\n",
        "        if valid_accuracy > best_accuracy_valid:\n",
        "            best_accuracy_valid = valid_accuracy\n",
        "            torch.save(model.state_dict(), f\"{info}_valid.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wfOeqnGEffV8"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model prediction probabilities, and return the list of probabilities, class predictions and sample paths\n",
        "def evaluate_proba(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_proba = []\n",
        "    cls_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Determine the class probabilities and predictions of the inputs\n",
        "            model_proba = F.softmax(outputs, dim=1)\n",
        "\n",
        "            for proba in model_proba:\n",
        "                cls_predict = torch.argmax(proba).item()\n",
        "                input_proba = max(proba[0].item(), proba[1].item())\n",
        "                all_proba.append(input_proba)\n",
        "                cls_predictions.append(cls_predict)\n",
        "\n",
        "    # The predicted class corresponds to the class of the largest predicted probability\n",
        "    idx = np.argmax(all_proba)\n",
        "    cls_prediction = cls_predictions[idx]\n",
        "\n",
        "    return all_proba, cls_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lion Optimizer Model\n",
        "[paper](https://arxiv.org/pdf/2302.06675)  \n",
        "Replicated from: [git hub](https://github.com/lucidrains/lion-pytorch/tree/main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "in3HIrVze8_y"
      },
      "outputs": [],
      "source": [
        "\"\"\"PyTorch implementation of the Lion optimizer.\"\"\"\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class Lion(Optimizer):\n",
        "  r\"\"\"Implements Lion algorithm.\"\"\"\n",
        "\n",
        "  def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
        "    \"\"\"Initialize the hyperparameters.\n",
        "\n",
        "    Args:\n",
        "      params (iterable): iterable of parameters to optimize or dicts defining\n",
        "        parameter groups\n",
        "      lr (float, optional): learning rate (default: 1e-4)\n",
        "      betas (Tuple[float, float], optional): coefficients used for computing\n",
        "        running averages of gradient and its square (default: (0.9, 0.99))\n",
        "      weight_decay (float, optional): weight decay coefficient (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    if not 0.0 <= lr:\n",
        "      raise ValueError('Invalid learning rate: {}'.format(lr))\n",
        "    if not 0.0 <= betas[0] < 1.0:\n",
        "      raise ValueError('Invalid beta parameter at index 0: {}'.format(betas[0]))\n",
        "    if not 0.0 <= betas[1] < 1.0:\n",
        "      raise ValueError('Invalid beta parameter at index 1: {}'.format(betas[1]))\n",
        "    defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "    super().__init__(params, defaults)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, closure=None):\n",
        "    \"\"\"Performs a single optimization step.\n",
        "\n",
        "    Args:\n",
        "      closure (callable, optional): A closure that reevaluates the model\n",
        "        and returns the loss.\n",
        "\n",
        "    Returns:\n",
        "      the loss.\n",
        "    \"\"\"\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        if p.grad is None:\n",
        "          continue\n",
        "\n",
        "        # Perform stepweight decay\n",
        "        p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "        grad = p.grad\n",
        "        state = self.state[p]\n",
        "        # State initialization\n",
        "        if len(state) == 0:\n",
        "          # Exponential moving average of gradient values\n",
        "          state['exp_avg'] = torch.zeros_like(p)\n",
        "\n",
        "        exp_avg = state['exp_avg']\n",
        "        beta1, beta2 = group['betas']\n",
        "\n",
        "        # Weight update\n",
        "        update = exp_avg * beta1 + grad * (1 - beta1)\n",
        "        p.add_(torch.sign(update), alpha=-group['lr'])\n",
        "        # Decay the momentum running average coefficient\n",
        "        exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXsT7PDRY6N-",
        "outputId": "1a20b223-7d82-44b6-8c51-b58a5e866adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "................................\n",
            " --- Epoch 1/30 - Train Loss: 0.6746 - Train Accuracy: 0.6580\n",
            "Validation Loss: 0.8719 - Validation Accuracy: 0.5000\n",
            "Confusion Matrix:\n",
            "[[ 0 36]\n",
            " [ 0 36]]\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "F1-score: 0.6666666666666666\n",
            "................................\n",
            " --- Epoch 2/30 - Train Loss: 0.6185 - Train Accuracy: 0.7260\n",
            "Validation Loss: 2.4849 - Validation Accuracy: 0.4583\n",
            "Confusion Matrix:\n",
            "[[ 9 27]\n",
            " [12 24]]\n",
            "Precision: 0.47058823529411764\n",
            "Recall: 0.6666666666666666\n",
            "F1-score: 0.5517241379310345\n",
            "................................\n",
            " --- Epoch 3/30 - Train Loss: 0.5256 - Train Accuracy: 0.7300\n",
            "Validation Loss: 0.9485 - Validation Accuracy: 0.6389\n",
            "Confusion Matrix:\n",
            "[[29  7]\n",
            " [19 17]]\n",
            "Precision: 0.7083333333333334\n",
            "Recall: 0.4722222222222222\n",
            "F1-score: 0.5666666666666667\n",
            "................................\n",
            " --- Epoch 4/30 - Train Loss: 0.4961 - Train Accuracy: 0.7620\n",
            "Validation Loss: 1.0320 - Validation Accuracy: 0.5833\n",
            "Confusion Matrix:\n",
            "[[30  6]\n",
            " [24 12]]\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.3333333333333333\n",
            "F1-score: 0.4444444444444444\n",
            "................................\n",
            " --- Epoch 5/30 - Train Loss: 0.5025 - Train Accuracy: 0.7720\n",
            "Validation Loss: 1.1781 - Validation Accuracy: 0.5556\n",
            "Confusion Matrix:\n",
            "[[31  5]\n",
            " [27  9]]\n",
            "Precision: 0.6428571428571429\n",
            "Recall: 0.25\n",
            "F1-score: 0.36\n",
            "................................\n",
            " --- Epoch 6/30 - Train Loss: 0.4724 - Train Accuracy: 0.7740\n",
            "Validation Loss: 0.6678 - Validation Accuracy: 0.5833\n",
            "Confusion Matrix:\n",
            "[[27  9]\n",
            " [21 15]]\n",
            "Precision: 0.625\n",
            "Recall: 0.4166666666666667\n",
            "F1-score: 0.5\n",
            "................................\n",
            " --- Epoch 7/30 - Train Loss: 0.5025 - Train Accuracy: 0.7820\n",
            "Validation Loss: 0.8416 - Validation Accuracy: 0.5833\n",
            "Confusion Matrix:\n",
            "[[31  5]\n",
            " [25 11]]\n",
            "Precision: 0.6875\n",
            "Recall: 0.3055555555555556\n",
            "F1-score: 0.42307692307692313\n",
            "................................\n",
            " --- Epoch 8/30 - Train Loss: 0.5039 - Train Accuracy: 0.7560\n",
            "Validation Loss: 0.7950 - Validation Accuracy: 0.5556\n",
            "Confusion Matrix:\n",
            "[[35  1]\n",
            " [31  5]]\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.1388888888888889\n",
            "F1-score: 0.2380952380952381\n",
            "................................\n",
            " --- Epoch 9/30 - Train Loss: 0.4527 - Train Accuracy: 0.7920\n",
            "Validation Loss: 0.7743 - Validation Accuracy: 0.5694\n",
            "Confusion Matrix:\n",
            "[[29  7]\n",
            " [24 12]]\n",
            "Precision: 0.631578947368421\n",
            "Recall: 0.3333333333333333\n",
            "F1-score: 0.4363636363636364\n",
            "................................\n",
            " --- Epoch 10/30 - Train Loss: 0.4545 - Train Accuracy: 0.8000\n",
            "Validation Loss: 0.8198 - Validation Accuracy: 0.6944\n",
            "Confusion Matrix:\n",
            "[[29  7]\n",
            " [15 21]]\n",
            "Precision: 0.75\n",
            "Recall: 0.5833333333333334\n",
            "F1-score: 0.6562499999999999\n",
            "................................\n",
            " --- Epoch 11/30 - Train Loss: 0.3851 - Train Accuracy: 0.8340\n",
            "Validation Loss: 0.8491 - Validation Accuracy: 0.5833\n",
            "Confusion Matrix:\n",
            "[[33  3]\n",
            " [27  9]]\n",
            "Precision: 0.75\n",
            "Recall: 0.25\n",
            "F1-score: 0.375\n",
            "................................\n",
            " --- Epoch 12/30 - Train Loss: 0.4242 - Train Accuracy: 0.8220\n",
            "Validation Loss: 0.7755 - Validation Accuracy: 0.6111\n",
            "Confusion Matrix:\n",
            "[[33  3]\n",
            " [25 11]]\n",
            "Precision: 0.7857142857142857\n",
            "Recall: 0.3055555555555556\n",
            "F1-score: 0.43999999999999995\n",
            "................................\n",
            " --- Epoch 13/30 - Train Loss: 0.3738 - Train Accuracy: 0.8480\n",
            "Validation Loss: 0.6485 - Validation Accuracy: 0.6667\n",
            "Confusion Matrix:\n",
            "[[31  5]\n",
            " [19 17]]\n",
            "Precision: 0.7727272727272727\n",
            "Recall: 0.4722222222222222\n",
            "F1-score: 0.5862068965517242\n",
            "................................\n",
            " --- Epoch 14/30 - Train Loss: 0.4593 - Train Accuracy: 0.8200\n",
            "Validation Loss: 0.6587 - Validation Accuracy: 0.6667\n",
            "Confusion Matrix:\n",
            "[[33  3]\n",
            " [21 15]]\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.4166666666666667\n",
            "F1-score: 0.5555555555555556\n",
            "................................\n",
            " --- Epoch 15/30 - Train Loss: 0.4258 - Train Accuracy: 0.8260\n",
            "Validation Loss: 0.7121 - Validation Accuracy: 0.6528\n",
            "Confusion Matrix:\n",
            "[[32  4]\n",
            " [21 15]]\n",
            "Precision: 0.7894736842105263\n",
            "Recall: 0.4166666666666667\n",
            "F1-score: 0.5454545454545454\n",
            "................................\n",
            " --- Epoch 16/30 - Train Loss: 0.3932 - Train Accuracy: 0.8220\n",
            "Validation Loss: 0.7059 - Validation Accuracy: 0.6250\n",
            "Confusion Matrix:\n",
            "[[32  4]\n",
            " [23 13]]\n",
            "Precision: 0.7647058823529411\n",
            "Recall: 0.3611111111111111\n",
            "F1-score: 0.490566037735849\n",
            "................................\n",
            " --- Epoch 17/30 - Train Loss: 0.3775 - Train Accuracy: 0.8440\n",
            "Validation Loss: 0.6772 - Validation Accuracy: 0.6528\n",
            "Confusion Matrix:\n",
            "[[33  3]\n",
            " [22 14]]\n",
            "Precision: 0.8235294117647058\n",
            "Recall: 0.3888888888888889\n",
            "F1-score: 0.5283018867924528\n",
            "................................\n",
            " --- Epoch 18/30 - Train Loss: 0.3567 - Train Accuracy: 0.8560\n",
            "Validation Loss: 0.8592 - Validation Accuracy: 0.6111\n",
            "Confusion Matrix:\n",
            "[[34  2]\n",
            " [26 10]]\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.2777777777777778\n",
            "F1-score: 0.4166666666666667\n",
            "................................\n",
            " --- Epoch 19/30 - Train Loss: 0.3337 - Train Accuracy: 0.8640\n",
            "Validation Loss: 0.7226 - Validation Accuracy: 0.6944\n",
            "Confusion Matrix:\n",
            "[[33  3]\n",
            " [19 17]]\n",
            "Precision: 0.85\n",
            "Recall: 0.4722222222222222\n",
            "F1-score: 0.6071428571428571\n",
            "................................\n",
            " --- Epoch 20/30 - Train Loss: 0.3367 - Train Accuracy: 0.8620\n",
            "Validation Loss: 0.6501 - Validation Accuracy: 0.6667\n",
            "Confusion Matrix:\n",
            "[[35  1]\n",
            " [23 13]]\n",
            "Precision: 0.9285714285714286\n",
            "Recall: 0.3611111111111111\n",
            "F1-score: 0.52\n",
            "................................\n",
            " --- Epoch 21/30 - Train Loss: 0.3349 - Train Accuracy: 0.8660\n",
            "Validation Loss: 0.7022 - Validation Accuracy: 0.6528\n",
            "Confusion Matrix:\n",
            "[[33  3]\n",
            " [22 14]]\n",
            "Precision: 0.8235294117647058\n",
            "Recall: 0.3888888888888889\n",
            "F1-score: 0.5283018867924528\n",
            "................................\n",
            " --- Epoch 22/30 - Train Loss: 0.3165 - Train Accuracy: 0.8780\n",
            "Validation Loss: 0.6860 - Validation Accuracy: 0.7222\n",
            "Confusion Matrix:\n",
            "[[29  7]\n",
            " [13 23]]\n",
            "Precision: 0.7666666666666667\n",
            "Recall: 0.6388888888888888\n",
            "F1-score: 0.696969696969697\n",
            "................................\n",
            " --- Epoch 23/30 - Train Loss: 0.2919 - Train Accuracy: 0.8840\n",
            "Validation Loss: 0.7506 - Validation Accuracy: 0.6944\n",
            "Confusion Matrix:\n",
            "[[21 15]\n",
            " [ 7 29]]\n",
            "Precision: 0.6590909090909091\n",
            "Recall: 0.8055555555555556\n",
            "F1-score: 0.7250000000000001\n",
            "................................\n",
            " --- Epoch 24/30 - Train Loss: 0.3042 - Train Accuracy: 0.8780\n",
            "Validation Loss: 0.5947 - Validation Accuracy: 0.7500\n",
            "Confusion Matrix:\n",
            "[[31  5]\n",
            " [13 23]]\n",
            "Precision: 0.8214285714285714\n",
            "Recall: 0.6388888888888888\n",
            "F1-score: 0.7187499999999999\n",
            "................................\n",
            " --- Epoch 25/30 - Train Loss: 0.2634 - Train Accuracy: 0.9080\n",
            "Validation Loss: 0.6032 - Validation Accuracy: 0.7361\n",
            "Confusion Matrix:\n",
            "[[31  5]\n",
            " [14 22]]\n",
            "Precision: 0.8148148148148148\n",
            "Recall: 0.6111111111111112\n",
            "F1-score: 0.6984126984126984\n",
            "................................\n",
            " --- Epoch 26/30 - Train Loss: 0.2622 - Train Accuracy: 0.9080\n",
            "Validation Loss: 0.6365 - Validation Accuracy: 0.7500\n",
            "Confusion Matrix:\n",
            "[[29  7]\n",
            " [11 25]]\n",
            "Precision: 0.78125\n",
            "Recall: 0.6944444444444444\n",
            "F1-score: 0.7352941176470588\n",
            "................................\n",
            " --- Epoch 27/30 - Train Loss: 0.2399 - Train Accuracy: 0.9160\n",
            "Validation Loss: 0.5605 - Validation Accuracy: 0.7917\n",
            "Confusion Matrix:\n",
            "[[34  2]\n",
            " [13 23]]\n",
            "Precision: 0.92\n",
            "Recall: 0.6388888888888888\n",
            "F1-score: 0.7540983606557377\n",
            "................................\n",
            " --- Epoch 28/30 - Train Loss: 0.2074 - Train Accuracy: 0.9240\n",
            "Validation Loss: 0.7726 - Validation Accuracy: 0.7361\n",
            "Confusion Matrix:\n",
            "[[32  4]\n",
            " [15 21]]\n",
            "Precision: 0.84\n",
            "Recall: 0.5833333333333334\n",
            "F1-score: 0.6885245901639344\n",
            "................................\n",
            " --- Epoch 29/30 - Train Loss: 0.2412 - Train Accuracy: 0.9280\n",
            "Validation Loss: 0.7004 - Validation Accuracy: 0.7778\n",
            "Confusion Matrix:\n",
            "[[36  0]\n",
            " [16 20]]\n",
            "Precision: 1.0\n",
            "Recall: 0.5555555555555556\n",
            "F1-score: 0.7142857142857143\n",
            "................................\n",
            " --- Epoch 30/30 - Train Loss: 0.2131 - Train Accuracy: 0.9100\n",
            "Validation Loss: 0.4930 - Validation Accuracy: 0.7639\n",
            "Confusion Matrix:\n",
            "[[30  6]\n",
            " [11 25]]\n",
            "Precision: 0.8064516129032258\n",
            "Recall: 0.6944444444444444\n",
            "F1-score: 0.746268656716418\n"
          ]
        }
      ],
      "source": [
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EfficientNetB0(num_classes=2, stochastic_depth_prob=0.2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Lion(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 30\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs, 'EfficientNetB0_Lion')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
